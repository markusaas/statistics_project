---
title: "ICY0006: Statistics and probabilty theory project"
output: html_notebook
---
Markus Aas 192920IVSB


### Stage 1: Information about the dataset and distribution graphs.
My chosen dataset is about [Apartment prices in Sao Paulo](https://www.kaggle.com/felipecabueno/apartment-prices-in-sao-paulo-br-2020), which I got from the website [Kaggle](https://www.kaggle.com/).

The dataset has 7 variables in total, of which 6 are numerical and 1 is categorical. The categorical variable in our case is

* neighborhood name

and the numerical variables are

* area of the apartment in square meters 
* number of bedrooms 
* number of bathrooms
* number of suites (set of rooms, usually bedroom with its own bathroom)
* number of parking spaces
* final price of the apartment in thousands of reais (R$, Brazilian currency).

Here is an example of the 10 first rows of my dataset and basic information about the dataset:
```{r}
apartments_sp <- read.csv("C:/Users/aasma/Statistics/apartments_sp.csv")
head(apartments_sp, n=10)
str(apartments_sp)
```
As we can see, we have 15703 different objects in this dataset.

The dataset has no missing values or errors.

The following histograms and frequency polygons give an approximate understanding of the variables in the dataset and showcase the frequency and values of those variables. In front of each histogram I have also given the range of the variable, because in histograms, when we have a certain value very few amount of times, it won't be showcased on histogram very well.
```{r}
library(ggplot2)

range(apartments_sp$square_meters)
ggplot(apartments_sp, aes(x=square_meters)) +
  geom_histogram(binwidth=10) +
  labs(title="Histogram of the area of the apartments", x="Square meters")

range(apartments_sp$bedrooms)
ggplot(apartments_sp, aes(x=bedrooms)) +
  geom_histogram(binwidth=1) +
  labs(title="Histogram of the number of bedrooms", x="Number of bedrooms")

range(apartments_sp$bathrooms)
ggplot(apartments_sp, aes(x=bathrooms)) +
  geom_histogram(binwidth=1) +
  labs(title="Histogram of the number of bathrooms", x="Number of bathrooms")

range(apartments_sp$suites)
ggplot(apartments_sp, aes(x=suites)) +
  geom_histogram(binwidth=1) +
  labs(title="Histogram of the number of suites", x="Number of suites")

range(apartments_sp$parking_spaces)
ggplot(apartments_sp, aes(x=parking_spaces)) +
  geom_histogram(binwidth=1) +
  labs(title="Histogram of the number of parking spaces", x="Number of parking spaces")

range(apartments_sp$final_price)
ggplot(apartments_sp, aes(x=final_price)) +
  geom_histogram(binwidth=100) +
  labs(title="Histogram of apartment prices", x="Thousands of reais (R$)")
```

For constructing histograms I have used ggplot2 library and I have constructed a histogram of each numerical variable in the dataset. Histograms let a person immediately grasp the sizes of our variables and give a rough image of the dataset. Especially with more discrete variables like the number of bedrooms or the number of bathrooms, where there are few unique values, we can have a histogram bar for each unique value.

The first histogram shows how many apartments with how big of an area there are. From the histogram we can see, that the most frequent size of an apartment is approximately somewhere between 60 and 70 square meters and the sizes go over 800 square meters.

The second histograms shows how many apartments with how many bedrooms are there. From the range we can see that we have apartments with 1-8 bedrooms and 3 bedrooms in one apartment being the most common one.

The third histogram shows the number of bathrooms in apartments. We know that the number of bathrooms in one apartment ranges from 1-10 and we can see from the histogram that the most common value is 2.

The fourth histogram shows the number of suites in apartments. Suites range from 0-8 and the most common value is 0.

The fifth histogram shows the number of parking spaces for the apartment, which range from 1-10 and the most common one is 1.

The sixth and last histogram shows the price for the apartments in thousands of reais. We can see, that the most common value for apartment prices is approximately somewhere between 400 000 and 600 000 reais with prices going up to 11 000 000 reais.

```{r}
library(ggplot2)

ggplot(apartments_sp, aes(x=square_meters)) +
  geom_freqpoly(size=0.8) +
  labs(title="Frequency polygon of area of the apartments", x="Square meters")

ggplot(apartments_sp, aes(x=bedrooms)) +
  geom_freqpoly(size=0.8) +
  labs(title="Frequency polygon of the number of bedrooms", x="Number of bedrooms")

ggplot(apartments_sp, aes(x=bathrooms)) +
  geom_freqpoly(size=0.8) +
  labs(title="Frequency polygon of the number of bathrooms", x="Number of bathrooms")

ggplot(apartments_sp, aes(x=suites)) +
  geom_freqpoly(size=0.8) +
  labs(title="Frequency polygon of the number of suites", x="Number of suites")

ggplot(apartments_sp, aes(x=parking_spaces)) +
  geom_freqpoly(size=0.8) +
  labs(title="Frequency polygon of the number of parking spaces", x="Number of parking spaces")

ggplot(apartments_sp, aes(x=final_price)) +
  geom_freqpoly(size=0.8) +
  labs(title="Frequency polygon of apartment prices", x="Thousands of reais (R$)")

```

For frequency polygons I also used the ggplot2 library. In case of more discrete variables frequency polygons showcase the data similarly to histograms, but in case of continuous variables, frequency polygons do not rely on separate segments in the graphs like histograms do with bars, but showcase the variable with one line. Maybe frequency polygons are better for grasping how the dataset looks like faster, but histograms make it more accurately.

Frequency polygons show exactly the same data here, they just showcase it in a different way.

### Stage 2: Central tendency and variability

In stage 2 I have constructed two tables, one for central tendency measures and one for variability measures.

```{r}
library(knitr)
df <- data.frame(
  Variable = c("Square meters", "Bedrooms", "Bathrooms", "Suites", "Parking spaces", "Final price"),
  Mean = c(mean(apartments_sp$square_meters), mean(apartments_sp$bedrooms), mean(apartments_sp$bathrooms), mean(apartments_sp$suites),
           mean(apartments_sp$parking_spaces), mean(apartments_sp$final_price)),
  Median = c(median(apartments_sp$square_meters), median(apartments_sp$bedrooms), median(apartments_sp$bathrooms), median(apartments_sp$suites),
             median(apartments_sp$parking_spaces), median(apartments_sp$final_price)),
  Mode = c(names(table(apartments_sp$square_meters))[table(apartments_sp$square_meters)==max(table(apartments_sp$square_meters))],
           names(table(apartments_sp$bedrooms))[table(apartments_sp$bedrooms)==max(table(apartments_sp$bedrooms))],
           names(table(apartments_sp$bathrooms))[table(apartments_sp$bathrooms)==max(table(apartments_sp$bathrooms))],
           names(table(apartments_sp$suites))[table(apartments_sp$suites)==max(table(apartments_sp$suites))],
           names(table(apartments_sp$parking_spaces))[table(apartments_sp$parking_spaces)==max(table(apartments_sp$parking_spaces))],
           names(table(apartments_sp$final_price))[table(apartments_sp$final_price)==max(table(apartments_sp$final_price))]),
  Mean_trimmed_20_percent = c(mean(apartments_sp$square_meters, trim = 0.1), mean(apartments_sp$bedrooms, trim = 0.1),
                              mean(apartments_sp$bathrooms, trim = 0.1), mean(apartments_sp$suites, trim = 0.1),
                              mean(apartments_sp$parking_spaces, trim = 0.1), mean(apartments_sp$final_price, trim = 0.1))
)
kable(df, caption="Central tendency table")
```

In central tendency table I have four variables

* mean
* median
* mode
* mean trimmed 20%

From this table we can see, what are the average values in our variables, the middle values, the most often occurring values and also averages, when 10% from both bottom and top is trimmed off.

The information in these tables corresponds to the information which we can roughly detect in histograms and frequency polygons. For example the easiest to compare are the modes, where we can see that the highest bars in histograms do in fact correspond to the numbers in this table. Rest of the information can also be seen to match with the graphs after a closer inspection.

I had decided to add the mean trimmed 20%, because some variables like square meters and final price can have some outliers, which can affect the mean maybe too much. After trimming 10% from both top and bottom of the variables we can see a decrease in the mean of square meters and final price, which is not that big of change, taking account their range, but still a change worth noting down. We can also see a quite significant change in mean in suites after trimming it, indicating the suites variable has outliers in the top end. When see the histogram and range of suites, we can see that we very small amount of suites in the range 5-8, which even doesn't appear on the histogram and that is the reason, why the trimmed mean is so much smaller than the normal mean.

```{r}
library(knitr)
df1 <- data.frame(
   Variable = c("Square meters", "Bedrooms", "Bathrooms", "Suites", "Parking spaces", "Final price"),
   Range = c(max(apartments_sp$square_meters) - min(apartments_sp$square_meters), max(apartments_sp$bedrooms) - min(apartments_sp$bedrooms),
             max(apartments_sp$bathrooms) - min(apartments_sp$bathrooms), max(apartments_sp$suites) - min(apartments_sp$suites),
             max(apartments_sp$parking_spaces) - min(apartments_sp$parking_spaces), max(apartments_sp$final_price) - min(apartments_sp$final_price)),
   Interquartile_range = c(IQR(apartments_sp$square_meters), IQR(apartments_sp$bedrooms), IQR(apartments_sp$bathrooms),
                           IQR(apartments_sp$suites), IQR(apartments_sp$parking_spaces), IQR(apartments_sp$final_price)),
   Standard_deviation = c(sd(apartments_sp$square_meters), sd(apartments_sp$bedrooms), sd(apartments_sp$bathrooms),
                          sd(apartments_sp$suites), sd(apartments_sp$parking_spaces), sd(apartments_sp$final_price)),
   Variance = c("-", var(apartments_sp$bedrooms), var(apartments_sp$bathrooms), var(apartments_sp$suites), var(apartments_sp$parking_spaces), "-")
)
kable(df1, caption="Variability table")
```



### Stage 3:

```{r}
library(knitr)

df2 <- apartments_sp[ , 2:7]

kable(cor(df2))

```

As we can see from the correlation matrix, all of the numerical values have linear relationships with each other from moderate to strong correlation.
So here are some scatter plots of variables with the highest correlation coefficients.

```{r}
library(ggplot2)

ggplot(apartments_sp, aes(x=square_meters, y=final_price)) +
  geom_point()

ggplot(apartments_sp, aes(x=bathrooms, y=suites)) +
  geom_jitter()

ggplot(apartments_sp, aes(x=square_meters, y=bathrooms)) +
  geom_jitter()

ggplot(apartments_sp, aes(x=bathrooms, y=parking_spaces)) +
  geom_jitter()

ggplot(apartments_sp, aes(x=square_meters, y=parking_spaces)) +
  geom_jitter()


```

For step 3 I had to choose the pair square_meters vs final_price, because these are the only continuous variables in my dataset, meaning I can only use these variables for linear regression.

```{r}
library(ggplot2)

lmMod <- lm(apartments_sp$final_price ~ apartments_sp$square_meters)
lmMod

ggplot(apartments_sp, aes(x=square_meters, y=final_price)) +
  geom_point() +
  geom_smooth(method=lm)

```

From the coefficients in the previous output we can construct our linear regression equation which in our case will be 
final_price = 59.908 + 7.308 * square_meters.


### Stage 4:

For stage 4 I chose the lottery EuroMillions.
Players have to choose 5 main numbers from 1 to 50 and 2 extra lucky star numbers from 1 to 12.

To compute the probabilites to win the largest prize, we are going to use the following formula on both the main numbers and lucky star numbers:
n! / (r! * (n - r)!)
where n is the total number of possible number and r is the number of numbers chosen.
So the computation in our case for main prize (5 main numbers guessed and 2 lucky star numbers guessed) would be as following:

```{r}
main_numbers <- factorial(50) / (factorial(5) * factorial(50 - 5))
lucky_stars <- factorial(12) / (factorial(2) * factorial(12 - 2))

main_numbers * lucky_stars

```
We have 1 in 139 838 160 chance of winning the main prize.

For odds for smaller winnings we have to use a more complicated formula:
(r! / (k! * (r - k)!)) * ((n-r)! / ((n - r) - (r - k))! * (r - k)!)
where k is the numbers chosen correctly.

So now that we have all the necessary formulas, we can compute the rest of the winning chances.
For simplicity I am not going to write the full formulas if computation are easy.

The computation for 5 main numbers and 1 lucky star number guessed:
```{r}
one_lucky_star <- lucky_stars / (2 * (factorial(10) / factorial(9)))

main_numbers * one_lucky_star
```
 The probability is 1 in 6 991 908.
 
 For 5 main numbers and  0 lucky star numbers:
```{r}
zero_lucky_starts <- lucky_stars / (factorial(10) / (factorial(8) * 2))

main_numbers * zero_lucky_starts
```
 The probability is 1 in 3 107 515.
 
 For 4 main numbers and 2 lucky star numbers:
```{r}
four_main_numbers <- main_numbers / ((factorial(5) / factorial(4)) * (factorial(45) / factorial(44)))
four_main_numbers * lucky_stars
```
 The probability is 1 in 621 503.
 
 For 4 main numbers and 1 lucky star number:
```{r}
four_main_numbers * one_lucky_star
```
 The probability is 1 in 31 075.
 
 For 3 main numbers and 2 lucky star numbers:
```{r}
three_main_numbers <- main_numbers / ((factorial(5) / 12) * (factorial(45) / (factorial(43) * 2)))
three_main_numbers * lucky_stars
```
The probability is 1 in 14 125.

For 4 main numbers and 0 lucky star numbers:
```{r}
four_main_numbers * zero_lucky_starts
```
The probability is 1 in 13811.

For 2 main number and 2 lucky star numbers:
```{r}
two_main_numbers <- main_numbers / ((factorial(5) / 12) * (factorial(45) / (factorial(42) * 6)))
two_main_numbers * lucky_stars
```
The probability is 1 in 985.

For 3 main numbers and 1 lucky star number:
```{r}
three_main_numbers * one_lucky_star
```
The probability is 1 in 706.

For 3 main numbers and 0 lucky star numbers:
```{r}
three_main_numbers * zero_lucky_starts
```
The probability is 1 in 314.

For 1 main number and 2 lucky star numbers:
```{r}
one_main_number <- main_numbers / ((factorial(5) / 24) * (factorial(45) / (factorial(41) * 24)))
one_main_number * lucky_stars
```
The probability is 1 in 188.

For 2 main numbers and 1 lucky star number:
```{r}
two_main_numbers * one_lucky_star
```
The probability is 1 in 49.

For 2 main numbers and 0 lucky star numbers:
```{r}
two_main_numbers * zero_lucky_starts
```
The probability is 1 in 22.

[EuroMillions wikipedia](https://en.wikipedia.org/wiki/EuroMillions)
[Formula source](https://www.wikihow.com/Calculate-Lotto-Odds)


### Stage 5:

In stage 5 first we are going to split the dataset into training and testing sets, with the training set being 80% of the whole dataset and testing set being 20%.

```{r}

dt <- apartments_sp[ , 2:7]
smp_siz = floor(0.80*nrow(dt))
set.seed(123)
train_ind = sample(seq_len(nrow(dt)), size = smp_siz)
train = dt[train_ind,]
test = dt[-train_ind,]
nrow(train)
nrow(test)

```

Now we are going to get the linear regression model from the training set:

```{r}

library(knitr)

kable(cor(train))

```

```{r}

library(ggplot2)

lmMod <- lm(train$final_price ~ train$square_meters)
lmMod

ggplot(train, aes(x=square_meters, y=final_price)) +
  geom_point() +
  geom_smooth(method=lm)

```

Linear regression equation for our training set is:
final_price = 62.081 + 7.276 * square_meters

```{r}
library(MLmetrics)

test_final_price <- c()

i <- 1

for (val in test$square_meters) {
  test_final_price[i] <- 62.081 + 7.276 * val
  i <- i + 1
}

test_training <- data.frame(test_final_price)

MAPE(test_training$test_final_price, test$final_price)
mean(abs((test$final_price - test_training$test_final_price) / test$final_price)) * 100
RMSE(test$final_price, test_training$test_final_price)
sqrt(mean((test$final_price - test_training$test_final_price)^2))
MAE(test_training$test_final_price, test$final_price)

```







